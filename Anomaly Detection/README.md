Anomaly Detection with PyOD

Project Description:
Anomaly detection involves identifying data points, items, or events that deviate from the expected patterns within a dataset, occurring infrequently but potentially indicating significant threats like cyber intrusions or fraud. This technique is vital for behavioral analysis, aiding in the detection, identification, and prediction of anomalies, also known as outlier detection. This project leverages the PyOD library, a comprehensive Python toolkit for identifying outliers in multivariate data using both unsupervised and supervised approaches, to perform anomaly detection across 17 diverse datasets stored in MATLAB (.mat) format.

The datasets, including arrhythmia, cardio, mnist, and shuttle, span various domains with sample sizes from 148 (lympho) to 49,097 (shuttle), dimensions from 6 (vertebral) to 274 (arrhythmia), and outlier percentages from 1.22% (satimage-2) to 35.90% (ionosphere). The project, implemented in a Jupyter Notebook (Anomaly_Detection_Project_Implementation_Python_Pyod.ipynb), evaluates 10 anomaly detection algorithms and explores combination frameworks to enhance detection performance. The objective is to compare model effectiveness using metrics like ROC-AUC, precision@n, and execution time, addressing challenges such as high dimensionality and varying outlier ratios to provide insights for applications like medical diagnostics and security.

My Work:
I developed this project to apply anomaly detection techniques to real-world datasets, showcasing my expertise in machine learning and data analysis. My contributions include:
1. Data Loading and Preprocessing: I loaded 17 .mat files using scipy.io.loadmat, extracting features (X) and labels (y). I standardized features with PyOD’s standardizer to normalize scales, critical for algorithms like KNN and OCSVM. I computed outlier percentages per dataset to assess their impact on detection, noting variations from low (musk at 3.17%) to high (pima at 34.90%), which influenced model performance.
2. Algorithm Implementation: I implemented 10 PyOD algorithms, categorized as:
Linear Models: Principal Component Analysis (PCA, using weighted projected distances to eigenvector hyperplanes), Minimum Covariance Determinant (MCD, using Mahalanobis distances), and One-Class SVM (OCSVM).
Proximity-Based Models: Local Outlier Factor (LOF), Clustering-Based Local Outlier Factor (CBLOF), K-Nearest Neighbors (KNN, using distance to kth neighbor), and Histogram-based Outlier Score (HBOS).
Probabilistic Model: Angle-Based Outlier Detection (ABOD).
Ensemble Models: Isolation Forest and Feature Bagging. I applied these to each dataset, defining ground truth (0 for inliers, 1 for outliers) to support both supervised and unsupervised evaluation.
3. Combination Frameworks: I explored four combination frameworks to enhance detection by aggregating base detector scores: Average (mean of all scores), Maximization (maximum score across detectors), Average of Maximum (AOM), and Maximum of Average (MOA). These frameworks aimed to improve robustness, particularly for datasets with complex patterns like mnist.
4. Performance Evaluation: I assessed models using ROC-AUC for overall detection accuracy, precision@n for top-ranked outlier accuracy, and execution time for efficiency. Results were stored in dataframes (roc_df, prn_df, time_df) for systematic analysis. For example, in the cardio dataset (1831 samples, 21 features), PCA achieved a ROC-AUC of 0.9616 and precision@n of 0.6849, while Isolation Forest scored 0.9527 ROC-AUC and 0.6027 precision@n. I also analyzed execution times, noting HBOS’s efficiency (0.0107s for cardio) versus MCD’s slower performance (21.8955s for shuttle).
5. Addressing Challenges: High-dimensional datasets (e.g., arrhythmia with 274 features) increased computational demands for ABOD (0.1575s), which I mitigated using PyOD’s optimized implementations. High outlier ratios in datasets like ionosphere challenged models, with KNN excelling (0.921 ROC-AUC). I handled MCD’s non-full-rank covariance issues (e.g., in cardio) by adjusting parameters. The combination frameworks helped address variability in individual model performance, particularly for datasets with low outlier percentages like satimage-2.
6. Analysis and Insights: I compared algorithm performance across datasets, finding PCA and Isolation Forest versatile, with PCA leveraging dimensionality reduction for high ROC-AUC (e.g., 0.9616 in cardio). HBOS was fast but less effective for high-dimensional data like mnist (0.1361 precision@n). Ensemble methods like Isolation Forest and CBLOF excelled in datasets with low outlier ratios (e.g., musk, 1.0 precision@n). The combination frameworks, particularly AOM and MOA, showed potential to enhance detection by balancing individual model strengths. For shuttle, HBOS’s 0.9985 precision@n highlighted its suitability for large datasets with moderate dimensions.
7. Key Findings: No single algorithm outperformed others universally; performance varied with dataset characteristics. PCA and Isolation Forest balanced accuracy and efficiency, while ABOD struggled with high-dimensional data. Combination frameworks improved robustness, suggesting their use for complex datasets. High outlier ratios required tailored approaches, and execution time analysis underscored trade-offs (e.g., HBOS’s speed vs. MCD’s depth). The project provides a foundation for applying anomaly detection in fields like healthcare (arrhythmia) and satellite imagery (satimage-2), with potential for further optimization using hybrid models or advanced preprocessing.

This project demonstrates my ability to manage diverse datasets, implement a wide range of anomaly detection algorithms, and derive actionable insights through rigorous evaluation. It highlights my proficiency in Python, PyOD, and analytical problem-solving, addressing real-world challenges in outlier detection.